{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page_of_players(url):\n",
    "    \"\"\"\n",
    "    This function takes a URL for a fifa index page and returns the list of players and their numeric ID\n",
    "    \"\"\"\n",
    "    # first we get the page\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # then we find the table with the player names in it\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('tbody')\n",
    "    rows = table.findAll('tr')\n",
    "    \n",
    "    # then we read each row of the table and extract the name and the player id and add it to our list\n",
    "    players_and_ids = []\n",
    "    for row in rows:\n",
    "        player_id = row.get_attribute_list('data-playerid')[0]\n",
    "        name = row.find('a').get_attribute_list('title')[0]\n",
    "        players_and_ids.append((name, player_id))\n",
    "        \n",
    "    # we finally return the list of players\n",
    "    return players_and_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_skill_cards(soup):\n",
    "    # the skills are separated into 'cards' of similar skills, so we need to get each card and read the skills listed\n",
    "    cards = soup.findAll('div', {'class':'col-12 col-md-4 item'})\n",
    "    skill_lists = {}\n",
    "    for card in cards:\n",
    "        # once we've got a card we read each line of it separately\n",
    "        skills = card.findAll('p')\n",
    "        for skill in skills:\n",
    "            if len(skill.findAll('span')) > 0:\n",
    "                # some skills are numeric - e.g. Sprint Speed is 87\n",
    "                skill_value = skill.findAll('span')[1].text\n",
    "                skill_name = skill.text[:-len(skill_value)-1]\n",
    "            else:\n",
    "                # but some are just characteristics - e.g good dribbler\n",
    "                # for these guys we give them a value of 1, as it's a boolean classification\n",
    "                skill_value = 1\n",
    "                skill_name = skill.text\n",
    "            skill_lists[skill_name] = skill_value\n",
    "                \n",
    "    return skill_lists\n",
    "\n",
    "def process_main_card(soup):\n",
    "    # the main skills section is all annoying custom stuff, so we've got to process each one individually\n",
    "    cards = soup.findAll('div', {'class':'card mb-5'})\n",
    "    main_player_card = cards[1]\n",
    "    \n",
    "    # the rating and potential are set up in the format [NAME] [RATING] [POTENTIAL]\n",
    "    rating, potential = main_player_card.find('h5').findAll('span')[0].text.split(' ')\n",
    "    \n",
    "    # The age is just a number in the fifth row\n",
    "    age = main_player_card.findAll('p')[4].findAll('span')[0].text\n",
    "    \n",
    "    # The work rates are in the form [ATTACK WR] / [DEFENSE WR], so we split on the spaces and just ignore the middle slash\n",
    "    attacking_work_rate, _, defensive_work_rate = main_player_card.findAll('p')[6].findAll('span')[0].text.split(' ')\n",
    "    \n",
    "    # Weak foot and skill moves are in the form of stars out of 5. the \"fas\" in the class indicates a filled star. we just count how many filled stars there are\n",
    "    weak_foot = len(main_player_card.findAll('p')[7].findAll('i', {'class':'fas fa-star fa-lg'}))\n",
    "    skill_moves = len(main_player_card.findAll('p')[8].findAll('i', {'class':'fas fa-star fa-lg'}))\n",
    "    \n",
    "    skill_lists = {\n",
    "        'rating':rating,\n",
    "        'potential':potential,\n",
    "        'age':age,\n",
    "        'attacking_work_rate':attacking_work_rate,\n",
    "        'defensive_work_rate':defensive_work_rate,\n",
    "        'weak_foot':weak_foot,\n",
    "        'skill_moves':skill_moves\n",
    "    }\n",
    "    \n",
    "    return skill_lists\n",
    "\n",
    "def process_player_page(player_url):\n",
    "    \"\"\"\n",
    "    This function takes a url and gets the skills listed on the fifa stats player page\n",
    "    \"\"\"\n",
    "    # get the web page\n",
    "    response = requests.get(player_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # extract the skill values\n",
    "    skill_cards = process_skill_cards(soup)\n",
    "    main_card = process_main_card(soup)\n",
    "    combined_skills = {**main_card, **skill_cards}\n",
    "    \n",
    "    return pd.Series(combined_skills)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded 1 of 20\n",
      "downloaded 2 of 20\n",
      "downloaded 3 of 20\n",
      "downloaded 4 of 20\n",
      "downloaded 5 of 20\n",
      "downloaded 6 of 20\n",
      "downloaded 7 of 20\n",
      "downloaded 8 of 20\n",
      "downloaded 9 of 20\n",
      "downloaded 10 of 20\n",
      "downloaded 11 of 20\n",
      "downloaded 12 of 20\n",
      "downloaded 13 of 20\n",
      "downloaded 14 of 20\n",
      "downloaded 15 of 20\n",
      "downloaded 16 of 20\n",
      "downloaded 17 of 20\n",
      "downloaded 18 of 20\n",
      "downloaded 19 of 20\n"
     ]
    }
   ],
   "source": [
    "list_of_players_and_ids = []\n",
    "# I don't know how many pages of players there are, so I just set a limit of 20\n",
    "max_n_pages_to_check = 20\n",
    "\n",
    "for i in range(1, max_n_pages_to_check + 1):\n",
    "    epl_players_url = f'https://www.fifaindex.com/players/{i}/?league=13&order=desc'\n",
    "    print(f'downloaded {i} of {max_n_pages_to_check}')\n",
    "    # We read each web page and extract the list of players\n",
    "    new_players = process_page_of_players(epl_players_url)\n",
    "    # And then update our list\n",
    "    list_of_players_and_ids = list_of_players_and_ids + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  out of 570\n",
      "10  out of 570\n",
      "20  out of 570\n",
      "30  out of 570\n",
      "40  out of 570\n",
      "50  out of 570\n",
      "60  out of 570\n",
      "70  out of 570\n",
      "80  out of 570\n",
      "90  out of 570\n",
      "100  out of 570\n",
      "110  out of 570\n",
      "120  out of 570\n",
      "130  out of 570\n",
      "140  out of 570\n",
      "150  out of 570\n",
      "160  out of 570\n",
      "170  out of 570\n",
      "180  out of 570\n",
      "190  out of 570\n",
      "200  out of 570\n",
      "210  out of 570\n",
      "220  out of 570\n",
      "230  out of 570\n",
      "240  out of 570\n",
      "250  out of 570\n",
      "260  out of 570\n",
      "270  out of 570\n",
      "280  out of 570\n",
      "290  out of 570\n",
      "300  out of 570\n",
      "310  out of 570\n",
      "320  out of 570\n",
      "330  out of 570\n",
      "340  out of 570\n",
      "350  out of 570\n",
      "360  out of 570\n",
      "370  out of 570\n",
      "380  out of 570\n",
      "390  out of 570\n",
      "400  out of 570\n",
      "410  out of 570\n",
      "420  out of 570\n",
      "430  out of 570\n",
      "440  out of 570\n",
      "450  out of 570\n",
      "460  out of 570\n",
      "470  out of 570\n",
      "480  out of 570\n",
      "490  out of 570\n",
      "500  out of 570\n",
      "510  out of 570\n",
      "520  out of 570\n",
      "530  out of 570\n",
      "540  out of 570\n",
      "550  out of 570\n",
      "560  out of 570\n"
     ]
    }
   ],
   "source": [
    "# Now we've got the id's we can download individual player level data\n",
    "i = 0 \n",
    "for name, id in list_of_players_and_ids:\n",
    "    player_url = f'https://www.fifaindex.com/player/{id}'\n",
    "    # Download and process the data\n",
    "    player_data = process_player_page(player_url)\n",
    "    \n",
    "    # Save it as a CSV\n",
    "    player_data.to_csv(rf'players/{name.replace(\" \", \"_\")}.csv')\n",
    "    \n",
    "    # This bit just prints a status update every 10 players\n",
    "    if i % 10 == 0 :\n",
    "        print(f'{i}  out of {len(list_of_players_and_ids)}')\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
